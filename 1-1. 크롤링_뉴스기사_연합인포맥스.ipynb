{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from random import *\n",
    "from datetime import timedelta, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'title':[],'date':[],'url':[],'script':[]}\n",
    "rr = randrange(3)\n",
    "    \n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "start_date = date(2017, 1, 4)\n",
    "end_date = date(2018, 1, 1)        \n",
    "        \n",
    "for day in daterange(start_date, end_date):\n",
    "    try:\n",
    "        \n",
    "        df = pd.DataFrame(raw_data)\n",
    "\n",
    "        driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "        driver.get(\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EA%B8%88%EB%A6%AC&nso=so%3Add%2Cp%3Afrom\" + day.strftime(\"%Y%m%d\") + \"to\" + day.strftime(\"%Y%m%d\") + \"%2Ca%3Aall0\")\n",
    "        driver.set_page_load_timeout(30)\n",
    "\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"news_popup\"]/a\"\"\").click()\n",
    "        time.sleep(rr)\n",
    "\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"ca_2227\"]\"\"\").click()\n",
    "        time.sleep(rr)\n",
    "\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"_nx_option_media\"]/div[2]/div[3]/button[1]/span\"\"\").click()\n",
    "        time.sleep(rr)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        a_tags = soup.find_all('a', class_='_sp_each_title')\n",
    "\n",
    "        tmp = []\n",
    "        tmp.append(a_tags)\n",
    "\n",
    "\n",
    "        page = driver.find_elements_by_xpath(\"\"\"//div[@class=\"paging\"]//a\"\"\")\n",
    "        time.sleep(rr)\n",
    "\n",
    "        for i in range(1, len(page)):\n",
    "\n",
    "            driver.get(\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EA%B8%88%EB%A6%AC&nso=so%3Add%2Cp%3Afrom\" + day.strftime(\"%Y%m%d\") + \"to\" + day.strftime(\"%Y%m%d\") + \"a:all&mynews=1&cluster_rank=60&start=\" + str(i*10+1) + \"&refresh_start=0\")\n",
    "            driver.set_page_load_timeout(30)\n",
    "\n",
    "\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            a_tags = soup.find_all('a', class_='_sp_each_title')\n",
    "            tmp.append(a_tags)\n",
    "            time.sleep(rr)\n",
    "\n",
    "\n",
    "        date=''\n",
    "        prepro=''\n",
    "        cnt= 1\n",
    "\n",
    "        for k in range(len(tmp)):\n",
    "            for j in tmp[k]:\n",
    "                #제목\n",
    "                print(\"제목 : \" + j.get_text())\n",
    "                title = j.get_text()\n",
    "                #URL\n",
    "                url = str(j['href'])\n",
    "                driver.get(url)\n",
    "                time.sleep(rr)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "             #   print(\"URL : \" + url)\n",
    "               #날짜\n",
    "                if soup.find('div', class_='info-text') is not None:\n",
    "                    date = soup.find('div', class_='info-text').find_all('li')\n",
    "                    date = str(date[1].text)\n",
    "                    date = date[4:14]\n",
    "             #       print(\"date : \"+ date)\n",
    "                #본문\n",
    "                if soup.find_all('div', id='article-view-content-div') is not None:\n",
    "                    div_tag = soup.find_all('div', id='article-view-content-div')\n",
    "                    prepro = str(div_tag)\n",
    "                    prepro = re.sub('\\n','',prepro,0).strip()\n",
    "                    prepro = re.sub('<!--.+?-->','',prepro,0).strip()\n",
    "                    prepro = re.sub('<.+?>','',prepro,0).strip()\n",
    "          #          print(prepro)\n",
    "                new_df = pd.DataFrame({'title':title, 'date':date, 'url':url, 'script':prepro}, index=[0])\n",
    "                df = pd.concat([df, new_df])\n",
    "        #        print('geeeee')\n",
    "                driver.close()\n",
    "                driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "            df.to_csv(\"infomax\" + day.strftime(\"%Y-%m-%d\") + \".txt\", header=True, index=False)\n",
    "\n",
    "    #    driver.close()\n",
    "    #    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        new_df = pd.DataFrame({'title':title, 'date':date, 'url':url, 'script':prepro}, index=[0])\n",
    "        df = pd.concat([df, new_df])\n",
    "        df.to_csv(\"infomax\" + day.strftime(\"%Y-%m-%d\") + \".txt\", header=True, index=False)\n",
    "        alert = driver.switch_to_alert()\n",
    "        alert.accept()\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
